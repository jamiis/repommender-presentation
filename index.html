<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>reveal.js - The HTML Presentation Framework</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.min.css">
		<link rel="stylesheet" href="css/theme/default.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', include the PDF print sheet -->
		<script>
			if( window.location.search.match( /print-pdf/gi ) ) {
				var link = document.createElement( 'link' );
				link.rel = 'stylesheet';
				link.type = 'text/css';
				link.href = 'css/print/pdf.css';
				document.getElementsByTagName( 'head' )[0].appendChild( link );
			}
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>Repommender!</h1>
					<p>
                    <small>A GitHub repository recommender. Name still under construction. I am Jamis.</small>
                    </p>
                    <p>
                    <small><a target="_blank" href="mailto:jmj2180@columbia.edu">jmj2180@columbia.edu</a> | <a target="_blank" href="http://github.com/jamiis">github.com/jamiis</a> | <a href="http://twitter.com/jamisjohnson">@jamisjohnson</a></small>
					</p>
				</section>

                <section>
                    <h2>Why?</h2>
                    <ol>
                        <li>It's Something I Would Use That Doesn't Yet Exist</li>
                        <li>Machine Learning Focued</li>
                    </ol>
                </section>

                <section>
                    <h2>Pilfering the Stars</h2>
                    <small><a href="http://www.githubarchive.org/">GitHub Archive</a> does not contain user-to-repo star data!</small>
                    <pre><code data-trim contenteditable>
#!/usr/bin/env python

from datetime import datetime as dt
from getpass import getpass
from time import sleep

import github3
from github3.exceptions import ForbiddenError, GitHubError, ServerError

# authenticate with github
uname = raw_input("Github username[jamiis]:") or "jamiis"
pwd = getpass()
gh = github3.login(uname, pwd)

def sleep_on_rate_limit(err):
    '''sleep until github api requests reset'''
    # note: dumb way to check error type but there is no rate limit specific err
    if err.message == "API rate limit exceeded for %s." % uname:
        # check rate limit every 5 minute
        retry_in = 60*5
        while True:
            if gh.rate_limit()['rate']['remaining']:
                return
            print 'continue in', retry_in/60.0, 'minutes'
            sleep(retry_in)

def prevail(gen):
    '''
    forces generator to continue even on error. very naive! don't use 
    unless you really want to brute force the generator to continue.
    '''
    while True:
        try:
            yield next(gen)
        except StopIteration:
            raise
        # catches all github3.exceptions
        except GitHubError as e:
            print e.__class__.__name__, e
            sleep_on_rate_limit(e)
            pass

with open('data/stargazers', 'a') as f:

    since = raw_input("Continue from repo ID[None]:") or None
    for user in prevail(gh.all_users(since=since)):

        print '%s\t%s' % (user.id, user.login)

        for repo in prevail(user.starred_repositories()):

            f.write("%s::%s::%s::%s\n" % (
                user.login,
                user.id,
                repo.full_name,
                repo.id))
                    </code></pre>
                </section>

                <section>
                    <h2>Pilfering the Stars</h2>
                    <pre><code data-trim contenteditable>
                    $ tail -f data/stargazers

                    rlenglet::126329::openstack/neutron::2400289
                    rlenglet::126329::rlenglet/openfaucet::1270260
                    rlenglet::126329::ubf/ubf-abnf::309879
                    rlenglet::126329::rlenglet/logtilla::305584
                    rlenglet::126329::ubf/ubf::173366
                    iotae::126330::iotae/Rails-Test::305579
                    iotae::126330::jrallison/authlogic_oauth::214049
                    iotae::126330::kalasjocke/authlogic-facebook-connect::190556
                    iotae::126330::binarylogic/authlogic_openid::156775
                    iotae::126330::insoshi/insoshi::6313
                    </code></pre>
                    <small>Approximately 4% of github's entire users population</small>
                </section>

                <section>
                    <h2>Architecture (In Theory and In Practice)</h2>
                    <ul>
                        <li>Spark cluster on EMR (four m3.large worker nodes and one m3.large master)</li>
                        <li>Load balanced EC-2 NodeJS/Express instances</li>
                        <li>DynamoDB for storing computed recommendations</li>
                        <li>EC-2 servers, NodeJS/Express</li>
                        <li>Pool of EC-2 workers scraping GitHub by rotating keys</li>
                    </ul>
                </section>

                <section>
                    <h2>Spark > Hadoop MapReduce</h2>
                    <ul>
                        <li>Spark does not write to disk (HDFS) between sequential map reduce jobs</li>
                        <li>Instead, RDDs are kept in memory between computations</li>
                        <li>Spark claims to be consistently 10-100x faster than Hadoop MR</li>
                        <li>Top level Apache project (2,402 stars on GitHub)</li>
                        <li>Written in Scala but you can interface with Java or Python</li>
                        <li>Built In Libraries: MLlib, Spark SQL (formerly Shark), GraphX, Spark Streaming
                    </ul>
                </section>

                <section>
                    <h2>Implicit Collaborative Filtering</h2>
                    <img src="lib/img/collaborative-filtering.jpg"
                         alt="collab" style="border: none; width: 750px;" />
                </section>

                <section>
                    <h2>Lessons Learned</h2>
                    <h3>Start Earlier!</h3>
                    <ul>
                        <li>Ramp up Spark</li>
                        <li>Running and refactoring the GitHub scrape job for a week</li>
                        <li>Learning MLlib and the ALS recommender</li>
                        <li>Efficacy. Big Data is not trivial: the cartesian product is huge!</li>
                        <li>Deploying and configuring Spark clusters</li>
                        <li>Tieing together multiple APIs</li>
                        <li>Fighting bugs</li>
                        <li>Frontend design</li>
                        <li>... etc., etc., etc.</li>
                    </ul>
                </section>

                <section>
                    <h2>Lessons Learned</h2>
                    <h3>In other words: <strong>Get a Team!</strong></h3>
                </section>

                <section>
                    <h2>Lessons Learned</h2>
                    <h3>Use Spark SQL</h3>
                    <p>If I got hit by a bus it would be very hard to reverse engineer non-Spark-SQL code</p>
                <pre><code data-trim contenteditable>
#!/usr/bin/env python

# https://spark.apache.org/docs/1.2.0/mllib-collaborative-filtering.html

from pyspark.mllib.recommendation import ALS
from numpy import array
from pyspark import SparkContext
from pprint import pprint
import os, sys

def extract_user_repo(line, fieldtype=float):
    line = line.split('::')
    if fieldtype is float:
        # user.id, repo.id
        fields = [line[1], line[3]]
    elif fieldtype in [unicode,str]:
        # user.login, repo.full_name
        fields = [line[0], line[2]]
    return array([fieldtype(f) for f in fields])

if __name__ == "__main__":
    sc = SparkContext(appName='TweetSentiment')

    env = os.getenv('ENV', 'dev')
    if  env == 'dev':
        lines = sys.argv[1] if len(sys.argv) >= 2 else '100'
        text = 'data/stargazers.%sk' % lines
    elif env == 'prod':
        text = 's3n://jamis.bucket/stargazers'

    # load and parse the text file
    data = sc.textFile(text)

    starpairs = data.map(extract_user_repo)
    starpairs.cache()

    users = starpairs.map(lambda t: t[0]).distinct()

    # get 5% most popular repos
    repos = starpairs.map(lambda t: t[1]).distinct()
    sample = int(0.01 * repos.count())
    top_repos = starpairs\
        .groupBy(lambda t: t[1])\
        .sortBy(lambda t: len(t[1]), False)\
        .map(lambda t: t[0])\
        .take(sample)
    top_repos_rdd = sc.parallelize(top_repos)
    top_repos_rdd.cache()
    top_repos_bc = sc.broadcast(top_repos)
    pprint(top_repos[:5])

    starpairs_filtered = starpairs.filter(lambda t: t[1] in top_repos_bc.value)
    starpairs_filtered.cache()

    # train recommendation model using alternating least squares
    stars_with_rating = starpairs_filtered.map(lambda t: array([t[0], t[1], 1]))
    model = ALS.trainImplicit(stars_with_rating, rank=1)

    # get all user->repo pairs without stars
    users_repos = users.cartesian(top_repos_rdd).groupByKey()
    stars_grouped = starpairs_filtered.groupByKey()
    unstarred = users_repos.join(stars_grouped)\
        .map(lambda i: (i[0], set(i[1][0]) - set(i[1][1]) ))\
        .flatMap(lambda i: [ (i[0], repo) for repo in i[1] ] )

    # predict unstarred user-repo pairs.
    predictions = model.predictAll(unstarred)

    # for each user, associate the 5 repos with the highest predicted rating.
    top = predictions\
        .map(lambda t: (t[0], (t[1],t[2])))\
        .groupByKey()\
        .map(lambda t: (t[0], [i[0] for i in sorted(t[1], key=lambda i: -i[1])[:5]]))\
        .coalesce(1)

    if  env == 'dev':
        top.saveAsTextFile('data/recommendations.%sk' % lines)
    elif env == 'prod':
        top.saveAsTextFile('s3n://jamis.bucket/recommendations')
                </code></pre>
                </section>
                </section>

                <section>
                  <h2>This Semester</h2>
                  <ul>
                        <li>EC2, DynamoDB, S3, SQS, ELB, ElasticBeanstalk</li>
                        <li>Spark (Scala, MLlib, pyspark)</li>
                        <li>Socket IO</li>
                        <li>Twitter, GoogleMaps, and GitHub APIs</li>
                        <li>CoffeeScript</li>
                        <li>Jade Templates</li>
                        <li>CSS framework Skel</li>
                  </ul>
                </section>

                <section>
                    <h1>Thank you!</h1>
                </section>

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.min.js"></script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				theme: Reveal.getQueryHash().theme || 'night', // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none

				// Parallax scrolling
                parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
                parallaxBackgroundSize: '2100px 900px',

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});

		</script>

	</body>
</html>
